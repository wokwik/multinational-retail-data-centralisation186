'''
Class DataCleaning

This script will contain a class DataCleaning with methods to clean data from each of the data sources.
'''

import pandas as pd
import numpy as np
from datetime import datetime
import re

from database_utils import DatabaseConnector
from data_extraction import DataExtractor

class DataCleaning:
    '''
    Description

    
    Parameters:
    ----------
    parm: type
        description
    
    Attributes:
    ----------
    attribute: type
        description

    Methods:
    -------
    method()
        description
    '''
    def __init__(self, parm1='', parm2=''):
        #pass

        self.parm1 = parm1
        self.parm2 = parm2
        self.attr1 = ''
        self.attr2 = ''

    def method1(self, parm1='') -> None:
        '''
        Description

        Parameters:
        ----------
        parm: type
            description

        Returns:
        ----------
        parm: type
            description
        '''
        pass
        #return self.attr1, self.attr2
    def clean_date(self,x):
        
        date = ''
        try:
            if type(x) == float and np.isnan(x):
                date = np.nan
            elif '-' in x:
                date = datetime.strptime(x, '%Y-%m-%d').date()
            elif '/' in x:
                date = datetime.strptime(x, '%Y/%m/%d').date()
            elif bool(re.search('\d{4}', x)):
                year = re.findall(r'\d{4}', x)[0]
                day = re.findall(r'\d{2}', x)[0]
                month = re.findall(r'[a-zA-Z]+', x)[0]
                date_str = year + '-' + month + '-' + day
                date = datetime.strptime(date_str, '%Y-%B-%d').date()
            else:
                # deals with x = '7KGJ3C5TSW', or not matching any of the above options
                date = np.nan
        except:
            # deals with errors when x='2328-BAER-23' i.e. does not convert to a proper/true date
            date = np.nan
        finally:
            return date
    
    def clean_user_data(self,df):
        # drop index column generated by salalchamy
        df.drop(['index'], axis=1, inplace=True)
        
        ## convert date strings to proper date type columns and resolve formatting and erronous values 
        #pd.to_datetime(df['join_date'], format='%Y-%m-%d') #won't work with wrong format or NA string values. so need custom apply()
        df['date_of_birth'] = df['date_of_birth'].apply(lambda x: self.clean_date(x))
        df['join_date'] = df['join_date'].apply(lambda x: self.clean_date(x))

        ## if email address does not contain @, then replace with NaN for removal in a later step
        df['email_address'] = df['email_address'].apply(lambda x: x if type(x) != float and '@' in x else np.nan)

        ## remove raws for both already existing and newly introduced NaN cells in the conversion above.
        dfc = df[~df.isnull().any(axis=1)].copy(deep=True) #df.dropna(how='any',axis=0, inplace=True) or use .copy() 

        return dfc
        #pass
    

    def clean_card_data(self, df):
        # clean date. erroneous and wrong dates are changed to null
        df['date_payment_confirmed'] = df['date_payment_confirmed'].apply(lambda x: self.clean_date(x))

        ## remove raws for both already existing and newly introduced NaN cells in the conversion above.
        dfc = df[~df.isnull().any(axis=1)].copy(deep=True)

        return dfc
        #pass

    def called_clean_store_data(self, df):
        # clean date. erroneous and wrong dates are changed to null
        df['opening_date'] = df['opening_date'].apply(lambda x: self.clean_date(x))

        # clean 3n9 for staff_numbers, which should be numeric. remove non numeric strings
        df['staff_numbers'] = df['staff_numbers'].apply(lambda x: re.sub('\D', '', x) if ~x.isnumeric() else x)
        
        # remove lat, which is empty and duplicate
        df.drop('lat', axis=1, inplace=True)

        # rearrange latitude next to longitude
        col = df.pop('latitude')
        df.insert(3, col.name, col)

        # ensure null latitude value for the web store is N/A similar to other not aplicable columns e.g. logitude
        df.loc[df['store_code'].str.contains('WEB') & df['latitude'].isna(), 'latitude'] = "N/A"
        # print(df[df['store_code'].str.contains('WEB') & df['longitude'].isna(), 'longitude'].values[0])
        # print(df[df['store_code'].str.contains('WEB') & df['latitude'].isna(), 'latitude'].values[0])

        ## remove raws for both already existing and newly introduced NaN cells in the conversion above.
        dfc = df.dropna(axis=0, subset=['opening_date']).copy(deep=True) # or use inplace=True
        #dfc = df[~df.isnull().any(axis=1)].copy(deep=True) #removes all data rows because lat column is all null i.e. empty for all rows!

        return dfc
    
    def clean_weight(self, string):
        # evaluate an equation in string format. as using eval() is not safe
        def make_numeric(equation):
            if '+' in equation:
                y = equation.split('+')
                x = int(y[0].strip())+int(y[1].strip()) if "." not in equation else float(y[0].strip())+float(y[1].strip())
            elif '-' in equation:
                y = equation.split('-')
                x = int(y[0].strip())-int(y[1].strip()) if "." not in equation else float(y[0].strip())-float(y[1].strip())
            elif 'x' in equation or '*' in equation:
                y = equation.split('x') if 'x' in equation else equation.split('*')
                x = int(y[0].strip())*int(y[1].strip()) if "." not in equation else float(y[0].strip())*float(y[1].strip())
            elif '/' in equation:
                y = equation.split('/')
                x = int(y[0].strip())/int(y[1].strip()) if "." not in equation else float(y[0].strip())/float(y[1].strip())
                
            return x

        matches = ["+", "-", "x", "*", "/"]
        #string = '5.3g'
        if 'kg' in string.lower() and not any(x in string for x in matches):
            string.lower().replace("kg", "").replace(',', '.')
            string = re.sub('[^0-9.]', '', string)
            val = float(string)
        #string = '100g'
        elif 'g' in string.lower() and not any(x in string for x in matches):
            string = string.lower().replace("g", "").replace(',', '.')
            string = re.sub('[^0-9.]', '', string)
            val = float(string)/1000
        #string = '100ml'
        elif 'ml' in string and not any(x in string for x in matches):
            string = string.lower().replace("ml", "").replace(',', '.')
            string = re.sub('[^0-9.]', '', string)
            val = float(string)/1000
        #string = '10oz' 
        elif 'oz' in string.lower() and not any(x in string for x in matches):
            #1oz = 28.4ml
            string = string.lower().replace("oz", "").replace(',', '.')
            string = re.sub('[^0-9.]', '', string)
            val = float(string)*28.4/1000
        #string = '2 x 50g'
        elif any(x in string for x in matches):
            kg = 'kg' in string.lower()
            g = 'kg' not in string.lower() and 'g' in string.lower()
            oz = 'oz' in string.lower()
            string = string.lower().replace("kg", "").replace("g", "").replace("ml", "").replace("oz", "").replace(",",".")
            val = make_numeric(string)
            val = val / 1000 if g == True else val
            val = val * 28.4 / 1000 if oz == True else val
        else:
            val = np.nan

        return val
    
    def convert_product_weights(self, df):
        # change weight values and change to normalised kg equivalents for all rows
        df['weight'] = df['weight'].apply(lambda x: self.clean_weight(str(x)))
        return df

    def clean_products_data(self, df):
        # name unamed column
        df.rename( columns={'Unnamed: 0':'index'}, inplace=True )

        # clean ¬£39.99
        df['product_price'] = df['product_price'].apply(lambda x: re.sub('[^0-9.]', '', x) if isinstance(x,str) else x)

        # clean date from none date or wrong and erronous formatted date
        df['date_added'] = df['date_added'].apply(lambda x: self.clean_date(x))

        ## remove raws for both already existing and newly introduced NaN cells in the conversion above.
        dfc = df[~df.isnull().any(axis=1)].copy(deep=True)

        return dfc
###
# run code
###
def run_warehouse_users():
    connector = DatabaseConnector()
    extractor = DataExtractor()
    cleaner = DataCleaning()

    db_names_list = connector.list_db_tables()

    for db_table in db_names_list:
        # ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']
        if db_table == 'legacy_users':
            print(f'\Retrieving DB Table :: {db_table} \n' )
            df_db = extractor.read_rds_table(connector, db_table, mode='remote')
            # print(table_load.head(5))
            
            print(f'\Table to CSV :: {db_table} - raw \n' )
            df_db.to_csv('./data/db__legacy_users_raw.csv', sep=',', index=False, header=True, encoding='utf-8')

            print(f'\Cleaning DB Table :: {db_table} \n' )
            dfc_users = cleaner.clean_user_data(df_db)
            #print(dfc.head(5))

            print(f'\Table to CSV :: {db_table} - clean \n' )
            dfc_users.to_csv('./data/db__legacy_users_clean.csv', sep=',', index=False, header=True, encoding='utf-8')
            
            print(f'\Table to Local DB :: {db_table} \n' )
            connector.upload_to_db(dfc_users,'dim_users')
    
    return

def run_warehouse_orders():
    connector = DatabaseConnector()
    extractor = DataExtractor()
    cleaner = DataCleaning()

    db_names_list = connector.list_db_tables()

    for db_table in db_names_list:
        # ['legacy_store_details', 'dim_card_details', 'legacy_users', 'orders_table']
        if db_table == 'orders_table':
            print(f'\Retrieving DB Table :: {db_table} \n' )
            df_db = extractor.read_rds_table(connector, db_table, mode='remote')
            print(df_db.head(5))
            
            print(f'\Table to CSV :: {db_table} - raw \n' )
            df_db.to_csv('./data/db__orders_raw.csv', sep=',', index=False, header=True, encoding='utf-8')

            #print(f'\Cleaning DB Table :: {db_table} \n' )
            #dfc_users = cleaner.clean_user_data(df_db)
            #print(dfc.head(5))

            #print(f'\Table to CSV :: {db_table} - clean \n' )
            #dfc_users.to_csv('./data/db__orders_clean.csv', sep=',', index=False, header=True, encoding='utf-8')
            
            #print(f'\Table to Local DB :: {db_table} \n' )
            #connector.upload_to_db(dfc_users,'dim_users')
    
    return

def run_pdf_cards_details():
    connector = DatabaseConnector()
    extractor = DataExtractor()
    cleaner = DataCleaning()

    pdf_path = 'https://data-handling-public.s3.eu-west-1.amazonaws.com/card_details.pdf'
    
    print(f'\Retrieving PDF :: {pdf_path} \n' )
    df_pdf = extractor.retrieve_pdf_data(pdf_path)
    
    print(f'\PDF to CSV :: {pdf_path} - raw \n' )
    df_pdf.to_csv('./data/pdf__card_details_raw.csv', sep=',', index=False, header=True, encoding='utf-8')

    print(f'\Cleaning PDF :: {pdf_path} \n' )
    dfc_cards = cleaner.clean_card_data(df_pdf)
    
    print(f'\PDF to CSV :: {pdf_path} - clean \n' )
    dfc_cards.to_csv('./data/pdf__card_details_clean.csv', sep=',', index=False, header=True, encoding='utf-8')

    print(f'\PDF to Local DB :: {pdf_path} \n' )
    connector.upload_to_db(dfc_cards,'dim_card_details')
    
    return

def run_api_stores():
    connector = DatabaseConnector()
    extractor = DataExtractor()
    cleaner = DataCleaning()

    print(f'\nReading API  :: Number of Stores \n' )
    num_stores = extractor.list_number_of_stores()
    print('Number of Stores ::', num_stores)
    
    print(f'\Retrieving API :: Individual Store Data \n' )
    stores_list = []
    for x in range(0,num_stores):
        store_data = extractor.retrieve_stores_data(x)
        stores_list.append(store_data)

    df_api = pd.DataFrame(stores_list)
    #print(df_api.head(5))

    print(f'\API to CSV :: Stores - raw \n' )
    df_api.to_csv('./data/api__stores_raw.csv', sep=',', index=False, header=True, encoding='utf-8')

    print(f'\Cleaning API :: Stores \n' )
    dfc_stores = cleaner.called_clean_store_data(df_api)

    # print('\nClean Stores ::')
    # print(dfc_stores.head(5))

    print(f'\API to CSV :: Stores - clean\n' )
    dfc_stores.to_csv('./data/api__stores_clean.csv', sep=',', index=False, header=True, encoding='utf-8')
    
    print(f'\API to Local DB :: Stores \n' )
    connector.upload_to_db(dfc_stores,'dim_store_details')
    
    return

def run_s3_products():
    connector = DatabaseConnector()
    extractor = DataExtractor()    
    cleaner = DataCleaning()

    print(f'\Retrieving S3 :: Products \n' )
    df_s3 = extractor.extract_from_s3()

    print(f'\S3 to CSV:: Stores - raw \n' )
    df_s3.to_csv('./data/s3__products_raw.csv', sep=',', index=False, header=True, encoding='utf-8')

    print(f'\Cleaning S3 :: Products \n' )
    dfkg_s3 = cleaner.convert_product_weights(df_s3)
    # print(dfkg_s3.head())
    # print(dfkg_s3.tail())
    dfc_products = cleaner.clean_products_data(dfkg_s3)

    print(f'\S3 to CSV:: Stores - clean \n' )
    dfc_products.to_csv('./data/s3__products_clean.csv', sep=',', index=False, header=True, encoding='utf-8')
    
    print(f'\S3 to Local DB :: Stores \n' )
    connector.upload_to_db(dfc_products,'dim_products')

    return

if __name__ == '__main__':
    #run_warehouse_users()
    #run_pdf_cards_details()
    #run_api_stores()
    #run_s3_products()
    run_warehouse_orders()

    #pass